{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Deep Learning Homework 2\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=None,\n",
    "            maxpool=True,\n",
    "            batch_norm=True,\n",
    "            dropout=0.0\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q2.1. Initialize convolution, maxpool, activation and dropout layers \n",
    "        \n",
    "        \n",
    "        # Q2.2 Initialize batchnorm layer \n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input for convolution is [b, c, w, h]\n",
    "        \n",
    "        # Implement execution of layers in right order\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout_prob, maxpool=True, batch_norm=True, conv_bias=True):\n",
    "        super(CNN, self).__init__()\n",
    "        channels = [3, 32, 64, 128]\n",
    "        fc1_out_dim = 1024\n",
    "        fc2_out_dim = 512\n",
    "        self.maxpool = maxpool\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        # Initialize convolutional blocks\n",
    "        \n",
    "        # Initialize layers for the MLP block\n",
    "        # For Q2.2 initalize batch normalization\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 3, 48, -1)\n",
    "\n",
    "        # Implement execution of convolutional blocks \n",
    "        \n",
    "        # Flattent output of the last conv block\n",
    "        \n",
    "        # Implement MLP part\n",
    "        \n",
    "        # For Q2.2 implement global averag pooling\n",
    "        \n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    " \n",
    "\n",
    "def train_batch(X, y, model, optimizer, criterion, **kwargs):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    model: a PyTorch defined model\n",
    "    optimizer: optimizer used in gradient step\n",
    "    criterion: loss function\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X, **kwargs)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def predict(model, X, return_scores=True):\n",
    "    \"\"\"X (n_examples x n_features)\"\"\"\n",
    "    scores = model(X)  # (n_examples x n_classes)\n",
    "    predicted_labels = scores.argmax(dim=-1)  # (n_examples)\n",
    "\n",
    "    if return_scores:\n",
    "        return predicted_labels, scores\n",
    "    else:\n",
    "        return predicted_labels\n",
    "\n",
    "\n",
    "def evaluate(model, X, y, criterion=None):\n",
    "    \"\"\"\n",
    "    X (n_examples x n_features)\n",
    "    y (n_examples): gold labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat, scores = predict(model, X, return_scores=True)\n",
    "        loss = criterion(scores, y)\n",
    "        n_correct = (y == y_hat).sum().item()\n",
    "        n_possible = float(y.shape[0])\n",
    "\n",
    "    return n_correct / n_possible, loss\n",
    "\n",
    "\n",
    "def plot(epochs, plottable, ylabel='', name=''):\n",
    "    plt.figure()#plt.clf()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(epochs, plottable)\n",
    "    plt.savefig('%s.pdf' % (name), bbox_inches='tight')\n",
    "\n",
    "\n",
    "def get_number_trainable_params(model):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def plot_file_name_sufix(opt, exlude):\n",
    "    \"\"\"\n",
    "    opt : options from argument parser\n",
    "    exlude : set of variable names to exlude from the sufix (e.g. \"device\")\n",
    "\n",
    "    \"\"\"\n",
    "    return '-'.join([str(value) for name, value in vars(opt).items() if name not in exlude])\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-epochs', default=40, type=int,\n",
    "                        help=\"\"\"Number of epochs to train for. You should not\n",
    "                        need to change this value for your plots.\"\"\")\n",
    "    parser.add_argument('-batch_size', default=8, type=int,\n",
    "                        help=\"Size of training batch.\")\n",
    "    parser.add_argument('-learning_rate', type=float, default=0.01,\n",
    "                        help=\"\"\"Learning rate for parameter updates\"\"\")\n",
    "    parser.add_argument('-l2_decay', type=float, default=0)\n",
    "    parser.add_argument('-dropout', type=float, default=0.1)\n",
    "    parser.add_argument('-optimizer',\n",
    "                        choices=['sgd', 'adam'], default='sgd')\n",
    "    parser.add_argument('-no_maxpool', action='store_true')\n",
    "    parser.add_argument('-no_batch_norm', action='store_true')\n",
    "    parser.add_argument('-data_path', type=str, default='intel_landscapes.v2.npz',)\n",
    "    parser.add_argument('-device', choices=['cpu', 'cuda', 'mps'], default='cpu')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    # Setting seed for reproducibility\n",
    "    utils.configure_seed(seed=42)\n",
    "\n",
    "    # Load data\n",
    "    data = utils.load_dataset(data_path=opt.data_path)\n",
    "    dataset = utils.ClassificationDataset(data)\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "    dev_X, dev_y = dataset.dev_X.to(opt.device), dataset.dev_y.to(opt.device)\n",
    "    test_X, test_y = dataset.test_X.to(opt.device), dataset.test_y.to(opt.device)\n",
    "\n",
    "    # initialize the model\n",
    "    model = CNN(\n",
    "        opt.dropout,\n",
    "        maxpool=not opt.no_maxpool,\n",
    "        batch_norm=not opt.no_batch_norm\n",
    "    ).to(opt.device)\n",
    "\n",
    "    # get an optimizer\n",
    "    optims = {\"adam\": torch.optim.Adam, \"sgd\": torch.optim.SGD}\n",
    "\n",
    "    optim_cls = optims[opt.optimizer]\n",
    "    optimizer = optim_cls(\n",
    "        model.parameters(), lr=opt.learning_rate, weight_decay=opt.l2_decay\n",
    "    )\n",
    "\n",
    "    # get a loss criterion\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # training loop\n",
    "    epochs = np.arange(1, opt.epochs + 1)\n",
    "    train_mean_losses = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    for ii in epochs:\n",
    "        print('\\nTraining epoch {}'.format(ii))\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            X_batch = X_batch.to(opt.device)\n",
    "            y_batch = y_batch.to(opt.device)\n",
    "            loss = train_batch(\n",
    "                X_batch, y_batch, model, optimizer, criterion)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "        mean_loss = torch.tensor(train_losses).mean().item()\n",
    "        print('Training loss: %.4f' % (mean_loss))\n",
    "\n",
    "        train_mean_losses.append(mean_loss)\n",
    "        val_acc, val_loss = evaluate(model, dev_X, dev_y, criterion)\n",
    "        valid_accs.append(val_acc)\n",
    "        print(\"Valid loss: %.4f\" % val_loss)\n",
    "        print('Valid acc: %.4f' % val_acc)\n",
    "\n",
    "    test_acc, _ = evaluate(model, test_X, test_y, criterion)\n",
    "    test_acc_perc = test_acc * 100\n",
    "    test_acc_str = '%.2f' % test_acc_perc\n",
    "    print('Final Test acc: %.4f' % test_acc)\n",
    "    # plot\n",
    "    sufix = plot_file_name_sufix(opt, exlude={'data_path', 'device'})\n",
    "\n",
    "    plot(epochs, train_mean_losses, ylabel='Loss', name='CNN-3-train-loss-{}-{}'.format(sufix, test_acc_str))\n",
    "    plot(epochs, valid_accs, ylabel='Accuracy', name='CNN-3-valid-accuracy-{}-{}'.format(sufix, test_acc_str))\n",
    "\n",
    "    print('Number of trainable parameters: ', get_number_trainable_params(model))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
